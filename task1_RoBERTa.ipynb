{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQ4MTry5n2Rn"
      },
      "outputs": [],
      "source": [
        "##############################################\n",
        "# PART 2: RoBERTa Model\n",
        "##############################################\n",
        "\n",
        "!pip install transformers datasets accelerate scikit-learn -q\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, get_cosine_schedule_with_warmup, DataCollatorWithPadding\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import f1_score\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device, torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"\")\n",
        "\n",
        "# Upload files (train.jsonl, val.jsonl, test.jsonl)\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Load data\n",
        "train_df = pd.read_json(\"train.jsonl\", lines=True)\n",
        "val_df = pd.read_json(\"val.jsonl\", lines=True)\n",
        "test_df = pd.read_json(\"test.jsonl\", lines=True)\n",
        "\n",
        "# Process labels\n",
        "train_df['tags'] = train_df['tags'].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
        "val_df['tags'] = val_df['tags'].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
        "label_map = {'phrase': 0, 'passage': 1, 'multi': 2}\n",
        "train_df['label'] = train_df['tags'].map(label_map)\n",
        "val_df['label'] = val_df['tags'].map(label_map)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "# Combine postText + targetTitle + first 4 paragraphs with RoBERTa separator\n",
        "def combine_text(row):\n",
        "    text = f\"{row['postText']} </s> {row['targetTitle']}\"\n",
        "    if isinstance(row['targetParagraphs'], list):\n",
        "        for para in row['targetParagraphs'][:4]:  # Take first 4 paragraphs\n",
        "            text += f\" </s> {para[:100]}\"  # Truncate each paragraph to 100 chars\n",
        "    return text\n",
        "\n",
        "train_df['text'] = train_df.apply(combine_text, axis=1)\n",
        "val_df['text'] = val_df.apply(combine_text, axis=1)\n",
        "test_df['text'] = test_df.apply(combine_text, axis=1)\n",
        "\n",
        "# Custom Dataset\n",
        "class SpoilerDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, include_labels=True):\n",
        "        self.texts = df['text'].tolist()\n",
        "        self.labels = df['label'].tolist() if include_labels else None\n",
        "        self.include_labels = include_labels\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            truncation=True,\n",
        "            max_length=384,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        item = {k: v.squeeze(0) for k, v in encoding.items()}\n",
        "        if self.include_labels:\n",
        "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "# Create datasets and loaders\n",
        "train_dataset = SpoilerDataset(train_df, tokenizer)\n",
        "val_dataset = SpoilerDataset(val_df, tokenizer)\n",
        "test_dataset = SpoilerDataset(test_df, tokenizer, include_labels=False)\n",
        "\n",
        "collator = DataCollatorWithPadding(tokenizer)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collator)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, collate_fn=collator)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, collate_fn=collator)\n",
        "\n",
        "# Initialize model with dropout\n",
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=3)\n",
        "model.config.hidden_dropout_prob = 0.3\n",
        "model.to(device)\n",
        "\n",
        "# Freeze first 6 layers\n",
        "for name, param in model.named_parameters():\n",
        "    if \"encoder.layer\" in name:\n",
        "        layer_num = int(name.split(\"encoder.layer.\")[1].split(\".\")[0])\n",
        "        if layer_num < 6:\n",
        "            param.requires_grad = False\n",
        "\n",
        "# Optimizer with Layer-wise Learning Rate Decay (LLRD)\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if \"encoder.layer.6\" in n], 'lr': 1e-5},\n",
        "    {'params': [p for n, p in model.named_parameters() if \"encoder.layer.7\" in n], 'lr': 1e-5},\n",
        "    {'params': [p for n, p in model.named_parameters() if \"classifier\" in n], 'lr': 3e-5},\n",
        "]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, weight_decay=0.01)\n",
        "\n",
        "# Scheduler with warmup\n",
        "num_training_steps = len(train_loader) * 8\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=500, num_training_steps=num_training_steps)\n",
        "\n",
        "# Mixed precision training scaler\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# Early stopping setup\n",
        "best_f1 = 0\n",
        "patience, patience_counter = 3, 0\n",
        "epochs = 8\n",
        "\n",
        "train_loss_list, val_f1_list = [], []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "        optimizer.zero_grad()\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        with torch.cuda.amp.autocast():\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    train_loss_list.append(avg_loss)\n",
        "    print(f\"Epoch {epoch+1}, Train Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    preds, true_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(**batch)\n",
        "            logits = outputs.logits\n",
        "            preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
        "            true_labels.extend(batch['labels'].cpu().numpy())\n",
        "\n",
        "    f1 = f1_score(true_labels, preds, average='macro')\n",
        "    val_f1_list.append(f1)\n",
        "    print(f\"Validation F1: {f1:.4f}\")\n",
        "\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), \"best_roberta.pt\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping!\")\n",
        "            break\n",
        "\n",
        "print(f\"Best Validation F1: {best_f1:.4f}\")\n",
        "\n",
        "# Plot training loss and validation F1\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, len(train_loss_list)+1), train_loss_list, marker='o', label='Train Loss', color='blue')\n",
        "plt.title('Training Loss per Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, len(val_f1_list)+1), val_f1_list, marker='o', label='Validation F1', color='green')\n",
        "plt.title('Validation F1 per Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('F1 Score')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Load best model and predict on test set\n",
        "model.load_state_dict(torch.load(\"best_roberta.pt\"))\n",
        "model.eval()\n",
        "test_preds = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        with torch.cuda.amp.autocast():\n",
        "            outputs = model(**batch)\n",
        "        test_preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
        "\n",
        "inv_label_map = {0:'phrase', 1:'passage', 2:'multi'}\n",
        "test_labels = [inv_label_map[i] for i in test_preds]\n",
        "\n",
        "# Save final submission CSV\n",
        "submission = pd.DataFrame({'id': test_df['id'], 'spoilerType': test_labels})\n",
        "submission.to_csv('prediction_task1.csv', index=False)\n",
        "\n",
        "files.download('prediction_task1.csv')\n"
      ]
    }
  ]
}